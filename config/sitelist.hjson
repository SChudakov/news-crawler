# This is a HJSON-File, so comments and so on can be used! See https://hjson.org/
# Furthermore this is first of all the actual config file, but as default just filled with examples.
{
  # Every URL has to be in an array-object in "base_urls".
  # The same URL in combination with the same crawler may only appear once in this array.
  "base_urls" : [
    {
      # Start crawling from faz.net
      "url": "https://www.unian.ua/",

      # Overwrite the default crawler and use th RecursiveCrawler instead
      "crawler": "RecursiveCrawler",

      # Because this site is weirt, use the
      # meta_contains_article_keyword-heuristic and disable all others because
      # overwrite will merge the defaults from "newscrawler.cfg" with
      # this
      "overwrite_heuristics": {
        "meta_contains_article_keyword": true,
        "og_type": false,
        "linked_headlines": false,
        "self_linked_headlines": false
      },
      # Also state that in the condition, all heuristics used in the condition
      # have to be activated in "overwrite_heuristics" (or default) as well.
      "pass_heuristics_condition": "meta_contains_article_keyword"
    },
    {
      # zeit.de has a blog which we do not want to crawl
      "url": "https://www.segodnya.ua/",

      "overwrite_heuristics": {
        # because we do not want to crawl that blog, disable all downloads from
        # subdomains
        "is_not_from_subdomain": true
      },
      # Update the condition as well, all the other heuristics are enabled in
      # newscrawler.cfg
      "pass_heuristics_condition": "is_not_from_subdomain and og_type and self_linked_headlines and linked_headlines"
    }
  ]
}
